/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package hello;

import static org.apache.spark.sql.functions.lit;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

public class App {
    public String getGreeting() {
        return "Hello World!";
    }

    public static void main(String[] args) {
        System.out.println(new App().getGreeting());
        System.out.println();

        SparkSession session = SparkSession.builder()
          .appName("Hello World")
          .master("local")
          .getOrCreate();

        Dataset<Row> df = session
          .read()
          .option("header", "true")
          .csv("names.csv");

        df = df.withColumn("city", lit("Tehran"));

        df.printSchema();

        long count = df.filter((Row r) -> r.getString(0).toLowerCase().startsWith("parham")).count();
        System.out.println("number of rows that has first column started with parham: " + count);

        System.out.println("data partitions: " + df.rdd().partitions().length);

        session.close();
    }
}
